{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchvision.datasets import ImageFolder, MNIST, CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_datalist(list):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.start_pos_data = []\n",
    "        super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = custom_datalist()\n",
    "for num_node in [9, 11]:\n",
    "    datalist.start_pos_data.append(len(datalist))   \n",
    "    for _ in range(1024):\n",
    "        edge_index = torch.from_numpy(\n",
    "            np.array(nx.fast_gnp_random_graph(num_node, 0.5).edges())\n",
    "        ).t().contiguous()\n",
    "        datalist.append(\n",
    "            Data(\n",
    "                x=torch.rand(num_node, 5), \n",
    "                edge_index=edge_index, \n",
    "                edge_attr=torch.rand(edge_index.size(1))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 128, number of nodes:11, test:11\n",
      "number of batches: 128, number of nodes:9, test:9\n",
      "number of batches: 128, number of nodes:11, test:11\n",
      "number of batches: 128, number of nodes:9, test:9\n",
      "number of batches: 128, number of nodes:9, test:9\n",
      "number of batches: 128, number of nodes:9, test:9\n",
      "number of batches: 128, number of nodes:11, test:11\n",
      "number of batches: 128, number of nodes:9, test:9\n",
      "number of batches: 128, number of nodes:9, test:9\n",
      "number of batches: 128, number of nodes:9, test:9\n",
      "number of batches: 128, number of nodes:11, test:11\n",
      "number of batches: 128, number of nodes:9, test:9\n",
      "number of batches: 128, number of nodes:11, test:11\n",
      "number of batches: 128, number of nodes:11, test:11\n",
      "number of batches: 128, number of nodes:11, test:11\n",
      "number of batches: 128, number of nodes:11, test:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13708/2050315575.py:7: UserWarning: __rfloordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  print(f\"number of batches: {batch_size}, number of nodes:{data.x.size(0)//batch_size}, test:{data.num_nodes // data.num_graphs}\")\n"
     ]
    }
   ],
   "source": [
    "from graphdataset import BucketSampler\n",
    "dataloader = DataLoader(\n",
    "    datalist, batch_sampler=BucketSampler(datalist, batch_size=128)\n",
    ")\n",
    "for data in dataloader:\n",
    "    batch_size = data.batch.max() + 1\n",
    "    print(f\"number of batches: {batch_size}, number of nodes:{data.x.size(0)//batch_size}, test:{data.num_nodes // data.num_graphs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1024]\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "bucketSampler = BucketSampler(datalist, batch_size=32)\n",
    "# print(bucketSampler.samplers)\n",
    "print(datalist.start_pos_data)\n",
    "for x in bucketSampler:\n",
    "    print(x.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_iterator(*iterables):\n",
    "    nexts = [iter(iterable).__next__ for iterable in iterables]\n",
    "    while nexts:\n",
    "        next = random.choice(nexts)\n",
    "        try:\n",
    "            yield next()\n",
    "        except StopIteration:\n",
    "            nexts.remove(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9, 11}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = set()\n",
    "for data in datalist:\n",
    "    num_nodes.add(data.num_nodes)\n",
    "num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalists = defaultdict(list)\n",
    "for data in datalist:\n",
    "    datalists[data.num_nodes].append(data)\n",
    "dataloaders = (\n",
    "    DataLoader(data, batch_size=128, shuffle=True) for data in datalists.values()\n",
    ")\n",
    "batches = get_combined_iterator(*dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in batches:\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2 # just for illustration\n",
    "for epoch_num in range(epochs):\n",
    "    print(f\"Epoch {epoch_num}\")\n",
    "    batches = get_combined_iterator(*dataloaders)\n",
    "    for batch in batches:\n",
    "        # now each graph in a batch will have the same number of nodes\n",
    "        # do training\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_tgff/multiple/train/raw'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_data_path = os.path.join(\"data_tgff\", \"multiple\", \"train\", \"raw\")\n",
    "root_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "dataloaders = []\n",
    "datas = []\n",
    "raw_file_names = [f for f in os.listdir(root_data_path) if os.path.isfile(os.path.join(root_data_path, f))]\n",
    "raw_file_names.sort(key=lambda f: int(re.split('_|[.]', f)[-2]))\n",
    "raw_file_names = raw_file_names[:2]\n",
    "for raw_file_name in raw_file_names:\n",
    "    raw_file_path = os.path.join(root_data_path, raw_file_name)\n",
    "    data = torch.load(raw_file_path)\n",
    "    datas.append(data)\n",
    "    dataloaders.append(DataLoader(data, batch_size=batch_size, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_flattened = list(itertools.chain(*datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas_flattened[0].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c982045a2c05bd0c9bf79125ebc2580bed822d4a312f410d45e33779c248249"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('venv_mtp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
