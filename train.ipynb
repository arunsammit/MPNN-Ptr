{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjfvEAFjGzPa"
      },
      "source": [
        "!git clone https://github.com/arunsammit/MPNN-Ptr mpnn_ptr\n",
        "%cd mpnn_ptr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdfJoa3jH8BH"
      },
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cu111.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iGeoT5pqx3HS"
      },
      "source": [
        "from models.mpnn_ptr import *\n",
        "from models.mpnn import *\n",
        "from models.seqToseq import *\n",
        "from torch import nn\n",
        "import torch\n",
        "from utils.utils import communication_cost, calculate_baseline"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd_nu5tox3HV"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3zKvw68x3HW"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zex2O7OCJmOj",
        "outputId": "c6dd34d1-77c5-4483-a0ec-0e3cd6df6c5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0mCC_GsJbmF",
        "outputId": "9d61cf4b-c252-4a1e-ed33-903055f783cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGqZMF4dIzo0"
      },
      "source": [
        "dataloader, distance_matrices = torch.load('drive/MyDrive/data_MTP/data_64.pt')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKM5nO4Px3HX",
        "outputId": "23c0b7c6-6a9c-4c31-b22b-80b08cc3a1a9"
      },
      "source": [
        "dataloader.batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFqoZKv6x3HY"
      },
      "source": [
        "max_graph_size = 64\n",
        "mpnn_ptr = MpnnPtr(input_dim=max_graph_size, embedding_dim=75, hidden_dim=81, K=3, n_layers=4,p_dropout=0, logit_clipping=True, device=device)\n",
        "mpnn_ptr.to(device)\n",
        "mpnn_ptr.apply(init_weights)\n",
        "optim = torch.optim.Adam(mpnn_ptr.parameters(), lr=0.0001)\n",
        "num_epochs = 3500\n",
        "epoch_penalty = torch.zeros(len(dataloader))\n",
        "loss_list_pre = []"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UoiYvg_x3HY",
        "outputId": "9d7fae5e-c907-4fe9-a12c-9a7fbdcae930"
      },
      "source": [
        "mpnn_ptr.train()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MpnnPtr(\n",
              "  (mpnn): Mpnn(\n",
              "    (node_init_embedding_layer): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=75, bias=False)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (theta4layers): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Linear(in_features=150, out_features=75, bias=False)\n",
              "        (1): ReLU()\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Linear(in_features=150, out_features=75, bias=False)\n",
              "        (1): ReLU()\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Linear(in_features=150, out_features=75, bias=False)\n",
              "        (1): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (theta5layers): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Linear(in_features=150, out_features=75, bias=False)\n",
              "        (1): ReLU()\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Linear(in_features=150, out_features=75, bias=False)\n",
              "        (1): ReLU()\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Linear(in_features=150, out_features=75, bias=False)\n",
              "        (1): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (theta6): Sequential(\n",
              "      (0): Linear(in_features=75, out_features=75, bias=False)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (theta7): Linear(in_features=150, out_features=75, bias=False)\n",
              "    (connection_embedding_layer): ConnectionsEmbedding(\n",
              "      (layer2): Sequential(\n",
              "        (0): Linear(in_features=65, out_features=74, bias=False)\n",
              "        (1): ReLU()\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Linear(in_features=75, out_features=75, bias=False)\n",
              "        (1): ReLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ptr_net): PointerNet(\n",
              "    (encoder): Encoder(\n",
              "      (rnn): LSTM(75, 81, num_layers=4)\n",
              "    )\n",
              "    (decoder): Decoder(\n",
              "      (rnn): LSTM(75, 81, num_layers=4)\n",
              "    )\n",
              "    (attn): Attention(\n",
              "      (attn): Sequential(\n",
              "        (0): Linear(in_features=162, out_features=81, bias=True)\n",
              "        (1): Tanh()\n",
              "      )\n",
              "      (v): Linear(in_features=81, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJHcqSRhL9YQ"
      },
      "source": [
        "mpnn_ptr = torch.load('./drive/MyDrive/data_MTP/model_64.pt')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nHXRexxQx3HZ",
        "outputId": "6f32de74-d2ee-4a1f-fa17-39dc34904918"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    epoch_penalty[:] = 0\n",
        "    for i, (data, distance_matrix) in enumerate(zip(dataloader, distance_matrices)):\n",
        "        num_samples = 16\n",
        "        \n",
        "        samples, predicted_mappings, log_likelihoods_sum = mpnn_ptr(data,num_samples)\n",
        "        # samples shape: (batch_size, num_samples, max_graph_size_in_batch)\n",
        "        # predicted_mappings shape: (batch_size, max_graph_size_in_batch)\n",
        "        # log_likelihoods_sum shape: (batch_size,)\n",
        "        penalty = communication_cost(data.edge_index, data.edge_attr, data.batch, data.num_graphs, distance_matrix, predicted_mappings)\n",
        "        epoch_penalty[i] = penalty.sum()\n",
        "        penalty_baseline = calculate_baseline(data.edge_index, data.edge_attr, data.batch, data.num_graphs, distance_matrix, samples)\n",
        "        loss = torch.mean((penalty.detach() - penalty_baseline.detach()) * log_likelihoods_sum)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(mpnn_ptr.parameters(), max_norm=1, norm_type=2)\n",
        "        optim.step()\n",
        "    batch_loss = epoch_penalty.sum().item()\n",
        "    loss_list_pre.append(batch_loss)\n",
        "    print('Epoch: {}/{}, Loss: {}'.format(epoch + 1, num_epochs, batch_loss))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1455/3500, Loss: 597615040.0\n",
            "Epoch: 1456/3500, Loss: 602834368.0\n",
            "Epoch: 1457/3500, Loss: 597316736.0\n",
            "Epoch: 1458/3500, Loss: 602626496.0\n",
            "Epoch: 1459/3500, Loss: 633705536.0\n",
            "Epoch: 1460/3500, Loss: 600915008.0\n",
            "Epoch: 1461/3500, Loss: 596480640.0\n",
            "Epoch: 1462/3500, Loss: 598476736.0\n",
            "Epoch: 1463/3500, Loss: 581659200.0\n",
            "Epoch: 1464/3500, Loss: 603349376.0\n",
            "Epoch: 1465/3500, Loss: 602249216.0\n",
            "Epoch: 1466/3500, Loss: 569025728.0\n",
            "Epoch: 1467/3500, Loss: 577299584.0\n",
            "Epoch: 1468/3500, Loss: 600713984.0\n",
            "Epoch: 1469/3500, Loss: 605289472.0\n",
            "Epoch: 1470/3500, Loss: 573447808.0\n",
            "Epoch: 1471/3500, Loss: 582188608.0\n",
            "Epoch: 1472/3500, Loss: 596497408.0\n",
            "Epoch: 1473/3500, Loss: 576987712.0\n",
            "Epoch: 1474/3500, Loss: 562592512.0\n",
            "Epoch: 1475/3500, Loss: 625937280.0\n",
            "Epoch: 1476/3500, Loss: 609946048.0\n",
            "Epoch: 1477/3500, Loss: 590579328.0\n",
            "Epoch: 1478/3500, Loss: 576181888.0\n",
            "Epoch: 1479/3500, Loss: 593129472.0\n",
            "Epoch: 1480/3500, Loss: 608574336.0\n",
            "Epoch: 1481/3500, Loss: 572056640.0\n",
            "Epoch: 1482/3500, Loss: 622383040.0\n",
            "Epoch: 1483/3500, Loss: 607554048.0\n",
            "Epoch: 1484/3500, Loss: 598075008.0\n",
            "Epoch: 1485/3500, Loss: 581225472.0\n",
            "Epoch: 1486/3500, Loss: 620683520.0\n",
            "Epoch: 1487/3500, Loss: 596442880.0\n",
            "Epoch: 1488/3500, Loss: 551885504.0\n",
            "Epoch: 1489/3500, Loss: 620884992.0\n",
            "Epoch: 1490/3500, Loss: 595966656.0\n",
            "Epoch: 1491/3500, Loss: 557572032.0\n",
            "Epoch: 1492/3500, Loss: 589718912.0\n",
            "Epoch: 1493/3500, Loss: 551932672.0\n",
            "Epoch: 1494/3500, Loss: 576554752.0\n",
            "Epoch: 1495/3500, Loss: 595392192.0\n",
            "Epoch: 1496/3500, Loss: 604850816.0\n",
            "Epoch: 1497/3500, Loss: 605387584.0\n",
            "Epoch: 1498/3500, Loss: 595827328.0\n",
            "Epoch: 1499/3500, Loss: 637182976.0\n",
            "Epoch: 1500/3500, Loss: 613791872.0\n",
            "Epoch: 1501/3500, Loss: 569333376.0\n",
            "Epoch: 1502/3500, Loss: 572265600.0\n",
            "Epoch: 1503/3500, Loss: 573465600.0\n",
            "Epoch: 1504/3500, Loss: 585336576.0\n",
            "Epoch: 1505/3500, Loss: 566973760.0\n",
            "Epoch: 1506/3500, Loss: 600973824.0\n",
            "Epoch: 1507/3500, Loss: 604720064.0\n",
            "Epoch: 1508/3500, Loss: 620897920.0\n",
            "Epoch: 1509/3500, Loss: 563693824.0\n",
            "Epoch: 1510/3500, Loss: 569614656.0\n",
            "Epoch: 1511/3500, Loss: 576969408.0\n",
            "Epoch: 1512/3500, Loss: 573788032.0\n",
            "Epoch: 1513/3500, Loss: 592802304.0\n",
            "Epoch: 1514/3500, Loss: 585083904.0\n",
            "Epoch: 1515/3500, Loss: 599871360.0\n",
            "Epoch: 1516/3500, Loss: 575661568.0\n",
            "Epoch: 1517/3500, Loss: 591080000.0\n",
            "Epoch: 1518/3500, Loss: 565516736.0\n",
            "Epoch: 1519/3500, Loss: 614204992.0\n",
            "Epoch: 1520/3500, Loss: 583539712.0\n",
            "Epoch: 1521/3500, Loss: 633476544.0\n",
            "Epoch: 1522/3500, Loss: 566628224.0\n",
            "Epoch: 1523/3500, Loss: 578033472.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e5cc78bcc91c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_mappings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihoods_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpnn_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# samples shape: (batch_size, num_samples, max_graph_size_in_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# predicted_mappings shape: (batch_size, max_graph_size_in_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mpnn_ptr/models/mpnn_ptr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, num_samples)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# batched_embeddings shape: (batch_size, max_num_nodes, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# pass embeddings and mask through PointerNet to get pointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mpredicted_mappings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihoods_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_mappings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihoods_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mpnn_ptr/models/seqToseq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mlog_probs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_decoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# logits shape: (batch, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mpnn_ptr/models/seqToseq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, cell)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# output shape: (batch, input_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 692\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhy398HyQ5wc"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYU1Lxm6Q7b9"
      },
      "source": [
        "torch.save(mpnn_ptr,'./drive/MyDrive/data_MTP/model_64.pt')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Rc7Ga5Qr0e"
      },
      "source": [
        "fig_pre, ax_pre = plt.subplots()  \n",
        "ax_pre.plot(loss_list_pre)  \n",
        "ax_pre.set_xlabel('number of epochs') \n",
        "ax_pre.set_ylabel('communication cost')  \n",
        "ax_pre.set_title(\"communication cost v/s number of epochs\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVlVriC4SPVZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}